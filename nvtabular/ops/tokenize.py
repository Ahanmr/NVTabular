from urllib.parse import urlparse

import cudf
import os
from cudf.utils.hash_vocab_utils import hash_vocab

from nvtabular.tag import Tag
from nvtabular.utils import download_file
from nvtabular.ops import Operator


class TokenizeText(Operator):
    """
    Tokenize a text column.

    Parameters
    ----------
    hash_file : str
        Path to hash file containing vocabulary of words with token-ids.
        This can be created from the raw vocabulary
        using the ``cudf.utils.hash_vocab_utils.hash_vocab`` function
    do_lower : bool, Default is True
        If set to True, original text will be lowercased before encoding.
    max_length : int
        Controls the maximum length to use or pad to.
    max_num_rows : int
        Maximum number of rows for the output token-ids expected to
        be generated by the tokenizer.
        Used for allocating temporary working memory on the GPU device.
        If the output generates a larger number of rows,
        behavior is undefined.
        This will vary based on stride, truncation, and max_length.
        For example, for non-overlapping sequences output rows will be
        the same as input rows.
        A good default can be twice the max_length
    add_special_tokens : bool, optional, defaults to True
        Whether or not to encode the sequences with the special tokens
        of the BERT classification model
    padding : "max_length"
        Pad to a maximum length specified with the argument max_length
    truncation : bool, defaults to False
        True:
        Truncate to a maximum length specified with the argument max_length
        False or 'do_not_truncate': default
        No truncation (Output differs from HuggingFace)
    stride : int, optional, defaults to 0
        The value of this argument defines the number of
        overlapping tokens.
        The information about the overlapping tokens is
        present in the metadata outputed.
    """

    def __init__(self, hash_file, max_length, do_lower=True, cache_dir=".tokenizers", **kwargs):
        if uri_validator(hash_file):
            hash_file = self.prepare_hash_file(hash_file, cache_dir)

        self.hash_file = hash_file
        self.do_lower = do_lower
        self.tokenizer_kwargs = kwargs
        self.max_length = max_length
        # self.tokenizer = SubwordTokenizer(hash_file, do_lower=do_lower)

    def transform(self, columns, gdf):
        for column in columns:
            tokens, masks, metadata = gdf[column].str.subword_tokenize(self.hash_file, max_length=self.max_length,
                                                                       **self.tokenizer_kwargs)
            gdf[column + "/tokens"] = cudf.DataFrame(tokens.reshape(-1, self.max_length)).values.tolist()
            gdf[column + "/attention_mask"] = cudf.DataFrame(masks.reshape(-1, self.max_length)).values.tolist()
            # gdf[column + "/metadata"] = cudf.DataFrame(metadata.reshape(-1, self.max_length)).values.tolist()

        return gdf

    def output_column_names(self, columns):
        return [
            *[column + "/tokens" for column in columns],
            *[column + "/attention_mask" for column in columns],
            # [column + "/metadata" for column in columns]
        ]

    def output_tags(self):
        return Tag.CATEGORICAL.value + Tag.TEXT_TOKENIZED.value

    @classmethod
    def prepare_hash_file(cls, tokenizer_url, cache_dir):
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)

        tokenizer_name = tokenizer_url.split("/")[-1]
        orig_file = os.path.join(cache_dir, tokenizer_name)
        hashed_file = os.path.join(cache_dir, "hashed_" + tokenizer_name)
        if not os.path.exists(hashed_file):
            download_file(tokenizer_url, orig_file)
            hash_vocab(orig_file, hashed_file)

        return hashed_file

    @classmethod
    def from_pretrained(cls, name, max_length, do_lower=True, cache_dir=".tokenizers", do_truncate=True, **kwargs):
        from transformers import AutoTokenizer

        vocab_file = AutoTokenizer.from_pretrained(name).pretrained_vocab_files_map["vocab_file"][name]

        return cls(vocab_file, max_length, do_lower=do_lower, cache_dir=cache_dir, do_truncate=do_truncate,
                   **kwargs)


def uri_validator(x):
    try:
        result = urlparse(x)
        return all([result.scheme, result.netloc, result.path])
    except:
        return False
